## [Overview](README.md)

## [Data Cleaning and Preparation](Data_Cleaning/Data_Cleaning.md)

## [Data Exploration](Data_Exploration/Data_Exploration.md)

## [Feature Selection](Feature_Selection/Feature_Selection.md)

# Models and Pipeline

## [Deep Learning](Deep_Learning/Deep_Learning.md)

#### Pipeline
After feature engineering, we concluded that the most correlated columns with our target variable are all categorical columns. So, we made a pipeline by defining three different stages:  
*	**StringIndexer**: Converts categorical values to categorical indices.  
*	**OneHotEncoder**: It maps categorical indices to a column of binary vector, with at most a single one-value per row.  
*	**VectorAssembler**: It combines raw features and features generated by different feature transformers into single feature vector.

```Python
# Defining Top 6 categorical features
categoricalColumns = ["deposit_type","agent","country","previous_cancellations" ,"market_segment", "total_of_special_requests"]

# Defining stages of pipeline for categorical columns 
stages = []
for categoricalCol in categoricalColumns:
    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + "Index")
    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], 
                            outputCols=[categoricalCol + "classEncoder"])
    stages += [stringIndexer, encoder]

# Indexing target variable column 'is_canceled'
label_stringIndx = StringIndexer(inputCol="is_canceled", outputCol="label")
stages += [label_stringIndx]

# Vectorizing all the categorical columns
from pyspark.ml.feature import VectorAssembler
Vector_Assembler=VectorAssembler(inputCols=['deposit_typeclassEncoder','agentclassEncoder',
                                           'countryclassEncoder','previous_cancellationsclassEncoder',
                                           'market_segmentclassEncoder','total_of_special_requestsclassEncoder'],
                                outputCol='features')
stages+=[Vector_Assembler]
```

Then, fitting the pipeline to the cleaned data-frame **Hotel_Bookings**
```Python
# Fitting all the above defined stages to the dataframe
partial_Pipeline = Pipeline().setStages(stages)
pipeline_Model = partial_Pipeline.fit(Hotel_Bookings)
prepped_DF = pipeline_Model.transform(Hotel_Bookings)
```

As the number of cancelled and uncancelled booking in dataset were not equally divided. In short, data was biased. So, for unbiased prediction, we divided the dataset into train and test with equal number of cancelled and uncancelled instances.  
```Python
# Defining train and test datasets
# The below code divides dataset into equal propotion of target variable for unbiased testing and training of models

zeros = prepped_DF.filter(prepped_DF["is_canceled"]==0)
ones = prepped_DF.filter(prepped_DF["is_canceled"]==1)
# split datasets into training and testing
train0, test0 = zeros.randomSplit([0.8,0.2], seed=1234)
train1, test1 = ones.randomSplit([0.8,0.2], seed=1234)
# stack datasets back together
train = train0.union(train1)
test = test0.union(test1)
```
Then, after splitting the data, we implemented three models:
* **GBT Classifier**
* **Decision Tree**
* **Random Forest**

#### Model Comparision
Our main objective in this project is to attain the best accuracy model for predicting whether a hotel booking can be cancelled in future or not. To reduce bias and classifying categorical variables at its best, we have employed Ensemble technique classifiers which are similar yet conceptually different. Below is the comparison of the output of the above-mentioned classifiers:  

|S.No.|	ModelName|	Accuracy|	Precision	|Recall|	ROC Curve|	PR curve|
|-----|--------|---|---------|-------|--------|--------|
1	|GBT	|0.8036|	0.7982|	0.9213|	0.8837|	0.8428|
2	|Decision Tree	|0.7912|	0.7768|	0.9383|	0.7225|	0.6967|
3	|Random Forest	|0.7637|	0.7285|	0.9963|	0.8112|	0.7697|  

As we can see that, **GBT Classifier** gives the best Accuracy and Precision of *80.36%* and *79.12%* respectively out of all the three models but the Recall is 88.37% which is least of all. Yet, we are choosing GBT as the Best Model for this problem because we believe that the major evaluation metrics for this kind of problem is Precision rather than Accuracy. If there is less precision in classifying the bookings with high probability of cancellation, the hotels can do overbooking and as a result they won’t be able to accommodate all customers together. The inability of hotel to serve their customers properly can also result in deterioration of hotel’s reputation in the market. That’ why we believe GBT is the best model for predicting hotel booking cancellation as it is giving the best precision value. 

